{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chrome Mixed Content Errors Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "##### This first cell is the the only cell you should need to make changes to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you'd like to reference a list of URLs, manual type them in this Python list\n",
    "url_list = ['https://example.com/page1.html',\n",
    "            'https://example.com/page2.html',\n",
    "            'https://example.com/page3.html']\n",
    "\n",
    "## If you'd like to loop through a CSV of URLs with URL in the 'url' column, paste that path here\n",
    "url_source = '/Users/you/Documents/my_urls.csv'\n",
    "\n",
    "## Designate the path where you'd like the output of results. It will have 3 columns, URL, severe_count, warning_count\n",
    "url_output = '/Users/you/Documents/my_urls_mixed_content_errors.csv'\n",
    "\n",
    "## Designate the local path of your Chromedriver. If you need to install: https://chromedriver.chromium.org/downloads\n",
    "chrome_path = '/Users/path/chromedriver'\n",
    "\n",
    "## As a fail safe, the script saves 10 rows of urls at a time to the output file, designate the amount of rows here. \n",
    "## You can make this slightly higher for CSVs over 10k URLs, but this script is untested above that threshold.\n",
    "rows_per_run = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actually create the blank output file as the path designated above\n",
    "df_output = pd.DataFrame(columns = ['url', 'severe_count', 'warning_count'])\n",
    "df_output.to_csv(url_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program\n",
    "##### The program will loop through each URL, extracting Mixed Content errors from the Chrome Console Log (webdriver.Chrome.get_log). \n",
    "##### It renders in full Chrome, including JavaScript, a rate of about 1-5 seconds per URL depending on host server speed and your internet connection. \n",
    "##### It has not been tested on a list larger than 10k URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable browser logging & set options\n",
    "## No changes are needed here\n",
    "\n",
    "d = DesiredCapabilities.CHROME\n",
    "d['loggingPrefs'] = { 'browser':'ALL' }\n",
    "\n",
    "opt = webdriver.ChromeOptions()\n",
    "opt.add_experimental_option('w3c', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = pd.read_csv(url_source)\n",
    "\n",
    "while len(df_source) > 0:\n",
    "    new_rows = df_source.iloc[ 0: rows_per_run, : ]\n",
    "    print(str(len(new_rows)) + ' rows to process')\n",
    "    url_list = new_rows['url'].tolist()\n",
    "    \n",
    "    console_output_df = pd.DataFrame()\n",
    "    \n",
    "    d = DesiredCapabilities.CHROME\n",
    "    d['loggingPrefs'] = { 'browser':'ALL' }\n",
    "    opt = webdriver.ChromeOptions()\n",
    "    opt.add_experimental_option('w3c', False)\n",
    "\n",
    "    for url in url_list:\n",
    "        driver = webdriver.Chrome(chrome_path, chrome_options=opt,desired_capabilities=d)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            console = driver.get_log('browser')\n",
    "\n",
    "            severe_count = 0\n",
    "            warning_count = 0\n",
    "\n",
    "            for log in console:\n",
    "                if \"Mixed Content\" in log['message'] and \"SEVERE\" in log['level']:\n",
    "                    severe_count += 1\n",
    "                if \"Mixed Content\" in log['message'] and \"WARNING\" in log['level']:\n",
    "                    warning_count += 1\n",
    "                print(window.__coverage__)\n",
    "\n",
    "            console_results = {'severe_count':severe_count, 'warning_count':warning_count, 'loaded':True}\n",
    "            console_row_df = pd.DataFrame(data=console_results, index=[0])\n",
    "            console_row_df['url'] = url\n",
    "            console_output_df = console_output_df.append(console_row_df, ignore_index=True, sort=False)\n",
    "\n",
    "            # Quit browser each time to avoid zombies\n",
    "            driver.quit()\n",
    "    \n",
    "        ## A failsafe to prevent URLs that won't load from blocking script from continuing\n",
    "        ## There may be a more elegant solution for this\n",
    "        except:\n",
    "            console_results = {'severe_count':'', 'warning_count':'', 'loaded':False}\n",
    "            console_row_df = pd.DataFrame(data=console_results, index=[0])\n",
    "            console_row_df['url'] = url\n",
    "            console_output_df = console_output_df.append(console_row_df, ignore_index=True, sort=False)\n",
    "\n",
    "            driver.quit()\n",
    "\n",
    "        \n",
    "    # Read the output CSV, write the new rows, then write the output back again\n",
    "    df_output = pd.read_csv(url_output)\n",
    "    df_output = df_output.append(console_output_df, ignore_index=True, sort=False)\n",
    "    df_output.to_csv(url_output, index=False)\n",
    "    \n",
    "    # If all the URLs were processed, write the source list back without the processed URLs\n",
    "    updated_df = df_source.iloc[ rows_per_run+1: , : ]\n",
    "    updated_df.to_csv(url_source, index=False)\n",
    "    df_source = pd.read_csv(url_source)\n",
    "    \n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
